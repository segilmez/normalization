\documentclass{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage{comment}
\usepackage{amssymb}
\usepackage{amsthm}

\newtheorem{definition}{Definition}
\newtheorem{property}{Property}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{example}{Example}
\newtheorem{notation}{Notation}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Algorithms and pseudo code
\usepackage[lined,boxed,linesnumbered]{algorithm2e}

%\usepackage1{verbatim}
%\usepackage{algorithm}
%\usepackage{algorithmicx}
%\usepackage{algpseudocode}

% Graphics and display
\usepackage{float}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{enumerate}
\usepackage{url}
\usepackage{multirow}

% Math symbols and environments
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd} % \varcurlyvee


\usepackage{calc}%#%
\usepackage{enumitem} %%% used in framed graph

\date{ }
% TikZ

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}
\usetikzlibrary{calc}

\usepackage{tikz-3dplot}

\definecolor{myyellow}{RGB}{255,255,150}
\definecolor{mylavender}{RGB}{125,249,255}
\definecolor{mygreen}{RGB}{144,238,14}
\definecolor{myred}{RGB}{255,0,0}

\newcommand\mytext[3][\scriptsize]{#2\\#1 #3}
\newcommand\mynode[4][]{%
  \node[mynode,#1,text width=\the\dimexpr#2cm] (#3) {\mytext{#3}{#4}}; 
}
\newcommand\mynot[4][]{%
  \node[mynot,#1,text width=\the\dimexpr#2cm] (#3) {\mytext{#3}{#4}}; 
}

\setcounter{secnumdepth}{4}

%%%%%%%%%%%%%%%%
%%%% Macros %%%%
%%%%%%%%%%%%%%%%

%%% Math
\newcommand{\nat}{\mathbb{N}}   % Natural numbers
\newcommand{\rat}{\mathbb{Q}}   % Rational numbers
\newcommand{\real}{\mathbb{R}}  % Real numbers
\newcommand{\runit}{[0, 1]}    % The real unit interval


% = with "hip." on the top, useful for indicating where a hypothesis comes in
\newcommand{\heq}{\stackrel{\text{\fontsize{3pt}{3pt}\selectfont hip.}}{=}}
% = because of the de Morgan laws.
\newcommand{\dmeq}{\stackrel{\text{\tiny{dM}}}{=}}


%%% Sets
\newcommand{\args}{\mathcal{A}} % Set of all arguments
\newcommand{\att}{\mathcal{R}}  % Set of all attacks
\newcommand{\valueset}{L}

\newcommand{\obj}{\mathcal{O}} % Set of all arguments and attacks as a union

%%% Votes on arguments
\newcommand{\varg}{V_{\args}}   % Function giving votes on arguments
\newcommand{\vargpro}[1]{\varg^+\left(#1\right)} % Pro votes on arguments
\newcommand{\vargcon}[1]{\varg^-\left(#1\right)} % Con votes on arguments
\newcommand{\vargtot}[1]{\varg^{max}\left(#1\right)} % Max votes on arguments

%%% Votes on attacks
\newcommand{\vatt}{V_{\att}}   % Function giving votes on attacks
\newcommand{\vattpro}[1]{\vatt^+\left(#1\right)} % Pro votes on attacks
\newcommand{\vattcon}[1]{\vatt^-\left(#1\right)} % Con votes on attacks

%%% Attack relations
% Attackers of a given argument
\newcommand{\attackers}[1]{\att^\text{-}\left(#1\right)} 
% Attackers of a given argument for the alternative framework F'
\newcommand{\altattackers}[1]{\att^{\prime\text{-}}\left(#1\right)}
% Ancestors of given argument according to the attack relation
\newcommand{\ancestors}[1]{\att^*\left(#1\right)} 

%%% Frameworks
\newcommand{\safid}{F}               % A single SAF, given by identifier
\newcommand{\safset}{\mathcal{F}}    % Set of all SAFs

\newcommand{\saf}{\safid = \safbody} % Framework id and respective tuple
%\newcommand{\safbody}{\langle \args, \att, \varg, \vatt \rangle} % SAF tuple
\newcommand{\safbody}{\langle \args, \att, V \rangle} % SAF tuple
\newcommand{\oldsaf}{\safid = \oldsafbody} % Ex Framework id and respective tuple
\newcommand{\oldsafbody}{\langle \args, \att, V \rangle} % old SAF tuple
% Alternative framework, same as \safbody but with ' everywhere ;)
\newcommand{\altsafbody}{\langle \args', \att', \varg', \vatt' \rangle} 

\newcommand{\safbodyO}{\langle \args, \att, \obj, V \rangle} % SAF tuple with objects
\newcommand{\safO}{\safid = \safbodyO} % Framework id and respective tuple with objects

%%% Semantics
\newcommand{\semid}{\mathcal{S}}        % Semantic framework identifier
% Semantic framework tuple
\newcommand{\sembody}{\left\langle \valueset,\SAFand_1, \SAFand_2,\SAFor,\lnot,\tau \right\rangle}
\newcommand{\semdef}{\semid = \sembody}     % Semantic framework id and tuple
\newcommand{\semprod}[1]{\semid^\cdot_{#1}} % Product semantic framework
\newcommand{\semsub}{\semid^\text{-}}       % Subtraction semantic framework
\newcommand{\semmax}{\semid^\text{max}}     % Max semantic framework

\newcommand{\sembodyNew}{\left\langle \valueset,\SAFand_\mathcal{A}, \SAFand_\mathcal{R},\SAFor,\lnot,\tau \right\rangle} %New semantic body
\newcommand{\sembodyNewE}{\left\langle \valueset,\SAFand_\mathcal{A}, \SAFand_\mathcal{R},\SAFor,\lnot,\tau_{e} \right\rangle} %New semantic body

\newcommand{\SAFand}{\curlywedge}     % Logical and for SAF equations 
\newcommand{\SAFor}{\curlyvee}        % Logical or for SAF equations
\DeclareMathOperator*{\SAFOr}{\bigcurlyvee} % Big or notation, works as \sum
                             %\varcurlyvee also works, but is smaller
\DeclareMathOperator*{\SAFAnd}{\bigcurlywedge} % Big and notation, works as \sum

\newcommand{\modelset}{\mathcal{M}}   % Set of all models


%#% old commands
\newcommand{\afit}{\textit{AF}}
\newcommand{\af}{\afit = \langle \args, \att \rangle}
\newcommand{\vote}{V}
\newcommand{\sem}{\mathcal{S}}

\newcommand{\ssv}{\mathcal{V}}
\newcommand{\tv}{\mathcal{T}}
\newcommand{\pv}{\mathcal{P}}
\newcommand{\xv}{X}
\newcommand{\ev}{\mathcal{E}}

\newcommand{\safit}{F}

\newcommand{\tupd}{\curlywedge}
\newcommand{\tatt}{\curlyvee}
\newcommand{\Tatt}{\varcurlyvee}

\newcommand{\argarray}{\{x_1, ..., x_n\}}

\newcommand{\voteset}{\mathcal{V}}
\newcommand{\vpro}{\vote^+}
\newcommand{\vcon}{\vote^-}

%%% Mappings
\newcommand{\mapping}{\Phi}

%%% Macro for framed graph
\newlist{tikzitem}{itemize}{1}
\setlist[tikzitem,1]{label=$\bullet$,nolistsep,leftmargin=*}

%%% Normalization related stuff
\newcommand{\dataset}{\mathcal{D}}   % Data set
\newcommand{\clusterset}{\mathcal{C}}   % Cluster set
\newcommand{\ssset}{\mathcal{T}}   % Social support set

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\date{}
\title{Normalization of Social Models in Social Argumentation Frameworks}
\maketitle

\section{Preliminaries}


\begin{definition}[Social argumentation frameworks]
A \emph{social argumentation framework} is a 3-tuple $\saf$, where
\begin{itemize}
  \item $\args$ is the set of arguments,
  \item $\att \subseteq \args \times \args$ is a binary attack relation between arguments,
  %\item $\obj = \args \cup \att$ is the set of objects, composed by the union of the sets of arguments and attack relations,
  \item $V : \args \cup \att \to \nat \times \nat$ is a voting function keeping the crowd's pro and con votes for each argument and attack relation.
\end{itemize}
\end{definition}

\begin{definition}[Vote Aggregation Function]
\label{def:voteAgg}

Given a totally ordered set $\valueset$ with top and bottom elements $\top$, $\bot$, and a framework $\saf$, $\tau$ is any function such that $\tau:  (\args \cup \att)  \times {2}^{ (\args \cup \att)} \to L$.
\end{definition}



\begin{definition}[Semantic Framework]
\label{def:semfram}
A semantic framework is a 6-tuple $\sem=\sembody$ where:

\begin{itemize}
  \item $\valueset$ is a totally ordered set with top and bottom elements $\top$, 
$\bot$, containing all possible valuations of an argument. 

  \item $\SAFand_\args,\SAFand_\att:\valueset\times \valueset\rightarrow \valueset$, are two binary algebraic operations used to restrict strengths to given values.
  
  \item $\SAFor:\valueset\times \valueset\rightarrow \valueset$, is a binary algebraic operation on argument valuations used to combine or aggregate valuations and strengths.
  
  \item $\lnot:\valueset\rightarrow \valueset$ is a unary algebraic operation for computing a restricting value corresponding to a given valuation or strength.
  
%  \item $\tau$ is a vote aggregation function which given some context, aggregates positive and negative votes into a social support value.

  \item $\tau$ is a vote aggregation function which, given the votes, determines the social support of an object within a set of objects.

\end{itemize}
\end{definition}

\begin{notation}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework. Then, let
\begin{itemize}
 \item $\obj = \args \cup \att$ be the set of objects, composed by the union of the sets of arguments and attack relations,
\item
\begin{itemize}
\item $V^{+} (o) \triangleq x$ denote the number of positive votes for object $o$,
\item $V^{-} (o) \triangleq y$ denote the number of negative votes for object $o$,
%S \item $v^r: \obj \to \real$ be a function s.t. $v^r(o) \triangleq \frac{x}{x + y}$,
\item $V^t: \obj \to \nat$ be a function s.t. $V^t(o) \triangleq x + y$,  
\end{itemize}
whenever $V(o) = (x, y)$,
\\
\item $V: 2^\obj \to 2^{\nat \times \nat}$ be a function s.t. $V(\mathcal{O}^{'}) = \{V(o)$ $|$ $o \in \mathcal{O}^{'}\}$,
\item $V^{t}: 2^\obj \to 2^{\nat \times \nat}$ be a function s.t. $V^t(\mathcal{O}^{'}) = \{V^t(o)$ $|$ $o \in \mathcal{O}^{'}\}$,
\item $max: 2^{\nat} \to \nat$ be a function s.t. it returns the maximum value amongst the natural numbers from the non-empty multiset given as the input.
\item $min: 2^{\nat} \to \nat$ be a function s.t. it returns the minimum value amongst the natural numbers from the non-empty multiset given as the input.
\item  $\tau:2^{\obj} \to 2^{L}$ be a function s.t. $\tau(A) = \{\tau(a)|a\in A\}$.%$\ssset \triangleq \{\tau(a, \args) | a \in \args$\} be the multiset of social support values of all arguments.
\item $\attackers{a} \triangleq \{a_i \in \args: (a_i, a) \in \att\}$ be the set of direct attackers of an argument $a \in \args$, 
\item$$\SAFOr_{x \in X} x \triangleq\left(\left(\left(  x_{1}\SAFor x_{2}\right) \SAFor...\right)\SAFor x_{n}\right)$$ $X=\left\{  x_{1},x_{2},...,x_{n}\right\}$ denote the aggregation of a multiset of elements of $\valueset$. 
\end{itemize}
\end{notation}

\begin{definition}[Model] 
\label{def:model}
  Let $\saf$ be a social argumentation framework, $\sem = \sembodyNew$ be a semantic framework. An $\semid$-model of $\safid$ is a total mapping $M : \args \rightarrow \valueset$ such that for all $a \in \args$,
  $$\displaystyle M(a) = \tau(a, \args ) \SAFand_\args \lnot \SAFOr_{a_i \in \attackers{a}} \left(\tau\left((a_i, a), \att \right) \SAFand_\att M\left(a_i\right)\right)$$
\end{definition}

\begin{definition}[Enhanced Vote Aggregation]
\label{def:enhVoteAgg}
Given a SAF $\saf$ and a semantic framework $\sem = \sembodyNew$, enhanced vote aggregation function
$\tau_{e}:\obj  \times {2}^{\obj} \rightarrow\lbrack0,1]$ is the vote aggregation function such that
\[
\tau_{e}  (o, \mathcal{O})  = \left\{
\begin{array}
[c]{lll}
0 &  & V(o) = (0,0)\\
\frac{V^{+}(o)}{V^{t}(o)+\frac{1}{max(V^{t}(o \cup \mathcal{O}))}} &  & \text{otherwise}%
\end{array}
\right.
\]
\end{definition}

\begin{definition}
[Enhanced Product Semantics]An enhanced product semantic framework is any
$\mathcal{S}_{e}^{\cdot}=\left\langle [0,1],\tau_{e
},\curlywedge^{\cdot}, \curlywedge^{\cdot}, \curlyvee^{\cdot},\lnot\right\rangle $ where 1) $x_{1}\curlywedge^{\cdot}
x_{2}=x_{1}\cdot x_{2}$, 2) $x_{1}\curlyvee^{\cdot} x_{2}=x_{1}+x_{2}-x_{1}\cdot
x_{2}$, 3) $\lnot x_{1}=1-x_{1}$
\end{definition}

\begin{conjecture}
\label{conj:1model}
Let $\saf$ be a social argumentation framework, $\mathcal{S}_{e}^{\cdot}=\left\langle [0,1],\tau_{e
},\curlywedge^{\cdot}, \curlywedge^{\cdot}, \curlyvee^{\cdot},\lnot\right\rangle$ be a semantic framework. Then, $\safid$ has one and only one $\semid_{e}^{.}-model.$

\end{conjecture}

\begin{notation}
Let $\saf$ be a social argumentation framework, $\sem = \sembodyNewE$ be a semantic framework. Then, under the assumption that Conjecture \ref{conj:1model} holds, let 
\begin{itemize}
\item $\dataset \triangleq \{M(a)|a \in \args$\} be the multiset of model evaluations of all arguments.
\end{itemize}
\end{notation}





%%PROPERTY ORNEGI%%
\begin{comment}
\begin{property}
\label{P1} [Absolute argument freeness] \\
Let $\tau$ be a a vote aggregation function given a set of values $\valueset$, a set of objects $\obj$ and a value function $V$. We say that $\tau$ is 'absolute argument free' if
% $X$ be a set of pairs of integers. %% <== sozlu hali
\begin{center}
$\forall o \in \obj$, $\tau (o, \obj) \neq \top$.
\end{center}
\end{property}
\end{comment}
%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{property}[]
%\end{property}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% SECTION on Normalization
\section{Normalization of Model Evaluations}

%\item $Y = \{\sigma(d) | a \in \dataset\}$


\subsection{Problem domain}
We have some dataset $\dataset$ containing the multiset of model evaluation values for all the arguments of a social abstract argumentation framework with product semantics. The values $\forall d \in \dataset$ lay between the interval $[0,1] \in \mathbb{R}$.

Under the aforementioned semantics, the values in our candidate datasets tend to form big clusters, especially close to zero. The main goal of this study is adjusting the original distributions by some mapping $\sigma: \dataset  \rightarrow  [0,1]$, in order to spread high-density areas (increasing the values in most cases), so that arguments with distinct values can be distinguished better.

The method of remapping may introduce some distortions or biases into the data. However in this case distortions are deliberately introduced to the system, in order to better expose the information content. 

\subsubsection{Characterization of Normalized Sets}

--Whole section Under Revision--

In this section we try to find out the parameters that define whether a dataset is \textit{normalized} or not. However the findings are at a preliminary level.

At this point, two values that I believe to be relevant are:
\begin{itemize}
\item The ratio between the range of the model evaluations and the range of the social support values

\begin{center}
 $\frac{max(\dataset) - min(\dataset)}{max(\ssset) - min(\ssset)}$
\end{center}
\item The ratio between the standard deviation of the model evaluations and the standard deviation of the social support values

\begin{center}
$\frac{\sigma(\dataset)}{\sigma(\ssset)}$
\end{center}
\end{itemize}

In the following subsection you may find the narrative of the thought process behind determining up with these two elements. But before that, below with Definition \ref{def:normSet} we take our first shot at defining what a normalized set is, by incorporating the aforementioned the parameters.


\begin{definition} [Normalized dataset]
\label{def:normSet}  
Given a SAF $\saf$ and a semantic framework $\sem = \sembodyNew$, the generic standard deviation mapping $\sigma: 2^{\nat} \rightarrow \nat$, the generic mean function $\mu: 2^{\nat} \rightarrow \nat$  and constants $r_1,  r_2 \in \real^+$, the multiset of model evaluations $\dataset$ is said to be normalized dataset if the following conditions hold:
\begin{itemize}
\item $r_1 - \frac{\mu(D)}{\mu(T)} \geq \dfrac{max(\dataset) - min(\dataset)}{max(\ssset) - min(\ssset)} \geq r_1 + \frac{\mu(D)}{\mu(T)}$, 
\item   $r_2 - \frac{\sigma(D)}{\sigma(T)} \geq \dfrac{\sigma(\dataset)}{\sigma(\ssset)} \geq r_2 + \frac{\sigma(D)}{\sigma(T)}$.
\end{itemize}
\end{definition}

%%PROPERTY ORNEGI%%
\begin{comment}
\begin{example}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\args =\{a_1, a_2, a_3\}$ and $\att =\{\}.$
\end{example}
\end{comment}
%%%%%%%%%%%%%

\subsubsection{Discussion on the Characterization of Normalized Sets}

Our initial suspicion was that whenever all model evaluations are very close to zero, this problem originates because the argumentation graph is strongly connected and this collection of datapoints $\dataset$ needs normalization. However consider the following setting:

\begin{example}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\args =\{a_1, a_2, a_3\}$ and $\att = \emptyset.$
\end{example}

In addition assume that all the social support values are extremely low. Since the attack set is empty, the model evaluation values for the arguments will be equal their social support values. So here the structure of the graph is not to blame. The model evaluations are low because of the popular opinion and thus probably the dataset should be left this way, it should not be normalized.
 
In order to exaggerate the example and display the situation clearer, we have taken the attack set to be empty. One may argue that to identify such a scenario it's enough to check how dense the argumentation graph is. However even if $\att$ all possible edges at full strength, the resulting model evaluations would be pretty close to the previous values since the social support is low for all the arguments. \emph{Thus the graph structure is not a reliable indicator in identifying whether a dataset that requires normalization.}

At this point I thought comparing the ranges of the social support values and the model evaluations might give us an idea. From Notation $1\&2$ please remember that $\ssset$ is the multiset of all social supports and $\dataset$  is the multiset of model evaluations of all arguments in the framework. In this particular example it would follow that  $\frac{max(\dataset) - min(\dataset)}{max(\ssset) - min(\ssset)} \simeq 1$. This would hint that the model evaluations have not shifted too far from the social support values. 

\vspace{3mm}
However it appears that considering the ranges by itself is not sufficient as well. To see this, consider the following example:

\begin{example}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\args =\{a_1, a_2, a_3\}$ and $\att = \{(a_1,a_2), (a_2,a_3), (a_3,a_1)\}.$
\end{example}

Assume all the arguments are at full strength. Then under product semantics the model evaluations would follow as $M(a_1) = M(a_2) = M(a_3) \simeq 0.33$.

Now let's add another argument to the system (also with perfect social support) that only attacks to the first argument:

\begin{example}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\args =\{a_1, a_2, a_3, a_4\}$ and $\att = \{(a_1,a_2), (a_2,a_3), (a_3,a_1), (a_4,a_1)\}.$
\end{example}

Now with the addition of a single argument and attack relation we have $M(a_4) = M(a_2) =  1$ and $M(a_1) = M(a_3) =  0$. The range of the social supports is the same, since all of them are at the same value. However the range of the model evaluations increased from a minimum of 0 to a maximum of 1. So on top of the comparison of ranges, we need some additional concept that would capture the essence of the distribution i.e. how datapoints are spread in the interval.

Since standard deviation measures how far a set of numbers is spread out, the relative measure of the two multisets' standard deviations, $\frac{\sigma(\dataset)}{\sigma(\ssset)}$ may overcome the shortcoming of only taking ranges into consideration. 

With these two ratio values, we obtain information on both the relative size of the sub-interval the datapoints are located at and also on the pattern they are spread. 

As you might see in Definition \ref{def:normSet}, we've tried to restrain the envisioned values of the two rations with some constants. Currently we've not settled on these values, and maybe in future we may choose to replace these constants with some other terms. 
%However I believe in order to be able to call a dataset \emph{normalized}, we should aim for both of these ratios to be close to 1. 

One thing that comes to mind is some sort of a supervized learning method. Perhaps we may assume a certain set of datasets as our training set. Then we may parameterize the constants with regards to our training set. That way we may fix the interval for the ratios of ranges and standard deviations. 

Let us see one example on this notion to make it more clear. For simplicity, we will only consider the standard deviation of the model evaluations when we are talking about the characterization of normalized sets (i.e. instead of comparing the ratios of range and standard deviations of social support values and model evaluations as we originally intend, we just take into account the standard deviations of the model evaluations in this example). 

\begin{example}
Assume we have the following three datasets that are known to be normalized:
\begin{itemize}
\item $D_1 = \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1\}$
\item $D_2 = \{0.05, 0.1, 0.3, 0.45, 0.75, 0.8, 0.9, 0.95\}$
\item $D_3 = \{0.25, 0.35, 0.4, 0.5, 0.6, 0.7, 0.75\}$
\end{itemize}
And we would like to figure out whether the following two data sets are normalized or not with respect to the training set.
 \begin{itemize}
\item $D_4 = \{0.05, 0.1, 0.1, 0.15, 0.2, 0.2, 0.85, 0.9, 0.95\}$
\item $D_5 = \{0.25, 0.3, 0.4, 0.6, 0.75, 0.9, 0.9\}$
\end{itemize}

Then the standard deviations for the training set follow as: $\sigma_1\cong 0.287, \sigma_2\cong 0.337, \sigma_3\cong 0.172$. 

Thus we may conclude that for a dataset $D_i$, the corresponding standard deviation value should fall into the interval $0.172 \leq \sigma_i \leq 0.337$ for $D_i$ to be regarded as normalized.

The standard deviations for the test set follow as: $\sigma_4\cong 0.364, \sigma_5\cong 0.254$.

Consequently we may determine that $D_5$ is a normalized dataset, on the other hand $D_4$ is not.

\end{example}

Undoubtedly we've to study this subject in more detail to find out the optimal range for these values. We might also uncover more parameters with respect to the notion of normalized sets. 

% We have some control on both the interval and the distribution bla bla

\subsection{Characterization of Normalizing Mappings}

The approach we adopt in this section is first defining a list of desirable properties that a normalizing mapping $\sigma_X: X  \rightarrow  [0,1]$ may possess, where $X$ is a fixed multiset.  
Then in the next section we will follow by defining some concrete classes of normalizing mappings which contain a subset the properties defined in the current section.

Before we move on, here we may better take a small pause to discuss the structure of the normalizing mapping. The mapping of each datapoint to a value in the unit interval is carried out with respect to the whole dataset. So the information that the dataset if fixed should be included in the mapping symbol. Indeed, that's what the subscript in the function definition above stands for. 

For example, assume two sets $A = \{1, 3 ,5, 7\}$ and $B = \{5, 50, 500, 5000\}$. Undoubtedly it would follow as $\sigma_A(5) \neq \sigma_B(5)$.


\vspace{3mm}
For the upcoming property definitions, let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework,  $M(a) = d \in \dataset$ be the model evaluation of an argument $a \in \args$, $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ a normalizing function with some metric  $\bigtriangleup: \valueset \times \valueset \rightarrow \real$.

\begin{property}  [Bottom argument] The argument of a context with the bottom value does not attain any strength in the normalized mutliset.
\begin{center}
$d = 0 \Rightarrow \sigma_{\dataset}(d) = 0$	%$M(a) = \bot \Rightarrow \sigma(M(a)) = 0$
\end{center}
\end{property}

\begin{property}  [Guarantee for the existence of arguments] An argument with some social strength is never diminished to the value of zero through normalization.
\begin{center}
$d \neq 0 \Rightarrow \sigma_{\dataset}(d) \neq 0$
\end{center}
\end{property}


\begin{comment} % TAKEN OUT - no assumption regarding the knowledge of \tau values exists in the current version
\begin{property}  [Decisiveness of Popular Opinion] Normalized value of a model evaluation is limited by the original argument's social support.
\begin{center}
$ \sigma_{\dataset}(d) \leq \tau(a, \args)$
\end{center}
\end{property}
\end{comment}


\begin{property} [Non-decreasing affect of normalization] Normalized value of a model evaluation should be bigger or equal the original value (in order to prevent the values cramming into a small space close to zero).
\begin{center}
$ d \leq\sigma_{\dataset}(d)$
\end{center}
\end{property}

\begin{property} [Conservation of relative ordering] The relative order between the pairs of normalized model evaluations is preserved.
\begin{center}
 $d_1 \geq d_2 \Longrightarrow \sigma_{\dataset}(d_1) \geq \sigma_{\dataset}(d_2)$\\%,  $d_1, d_2 \in \dataset$, \\
\end{center}
\end{property}

\begin{property} [Conservation of distance ordering] The relative order between the distance of pairs of normalized model evaluations is preserved. 
\label{prop:ConDis}
\begin{center}
 $\bigtriangleup(d_1, d_2) \geq \bigtriangleup(d_3, d_4) \Longrightarrow 
\bigtriangleup(\sigma_{\dataset}(d_1), \sigma_{\dataset}(d_2)) \geq \bigtriangleup(\sigma_{\dataset}(d_3), \sigma_{\dataset}(d_4))$\\%,  $d_1, d_2 \in \dataset$, \\
\end{center}
\end{property}


Even all together, these properties still give way to very flexible mapping definitions. For instance the identity mapping $id: x \mapsto x$ would interestingly satisfy all of the aforementioned properties.

Thus we have to define some property based on a concept that could capture the essence of specifically dense areas of the set of model evaluations. In the light of this, we benefit from the study of cluster analysis.

Very crudely, our objective via utilizing cluster analysis is to identify groups of objects that are very similar with regard to their values. Before going into more detail, firstly we need to formally define what a \textit{cluster} is.  In the literature, there are many definitions with respect to clusters. In our context we will  define them closely to the \textit{partitions} from the classical set theory via updating the definition accordingly with respect to multisets. 

\begin{definition} [Cluster]
A family of multisets $\clusterset$ is a clustering of a multiset $X$ and every element C of $\clusterset$ is a cluster if and only if all of the following conditions hold:

\begin{itemize}
\item Set of clusters does not contain the empty set.
\begin{center}
$\emptyset \notin \clusterset$
\end{center}
\item (Collectively exhaustive) The sum of the multisets in $\clusterset$ is equal to $X$. 
\begin{center}
$\biguplus _{C \in \clusterset}C = X$
\end{center} 
\item (Mutually exclusive) Distinct multisets do not contain shared elements. %The intersection of any two distinct multisets in $\clusterset$ is empty
\begin{center}
$(C_i, C_j \in \clusterset) \wedge (C_i \neq C_j) \Longrightarrow (C_i \wedge C_j = \emptyset)$ 
%$\forall x \in X$ if $x\in C_i$ then $x \notin C_j$, when $i \neq j$
\end{center}

\end{itemize}
\end{definition}

As mentioned clustering is the task of grouping a set of objects in such a way that objects in the same cluster are more similar (in some sense or another) to each other than to those in other cluster. So we should also define the concept of \textit{similarity} in a formal manner. Different procedures adopt different metrics, $\bigtriangleup: S \times S \rightarrow \real$ where $S$ is some set of valuations, when grouping the most similar objects into clusters. We have a single clustering variable, the model evaluation of arguments. Thus using the generic distance function as the similarity measure is a possibility. 

We continue by stating two more preliminary definitions that we will utilize in our last property. 


\begin{definition} [Separated points] 
Let $\clusterset$ be a clustering of some multiset X and $C_i, C_j \in \clusterset$. Two distinct datapoints $p, q$ are said to be separated if $p \in C_i$ and  $q \in C_j$ when $i \neq j$. 
\end{definition}

\begin{notation} 
Let $Separated(p,q)$ be the shortcut of notation where $p$ and $q$ are two separated points from some clustering $\clusterset$.
\end{notation}

\begin{definition} [Spacing] 
Given a metric $\bigtriangleup$, the spacing of a clustering $\clusterset$ is the minimum distance between any two separated points:

$\bigtriangleup(X, Y) = min_{x\in X, y\in Y} \bigtriangleup(x,y)    $	% $min_{Separated (p,q)}\bigtriangleup(p,q)$.
where X and Y are any two distinct clusters, and d(x,y) denotes the distance between the two elements x and y.

\end{definition}

One last discussion before we continue to the last property is $k$, the number of clusters. Some clustering procedures require $k$ to be defined by the user manually, and the rest compute it as the starting step of the procedure. The problem with the family of clustering methods that compute the number of clusters by themselves is that they don't scale well. They have a high time-complexity, in all cases exceeding the cubic time. Thus we may either use a method like Monte Carlo to generate an artificial $k$ and clustering centers then continue with the efficient methods, or use the inefficient but \textit{self-computing} methods initially and then switch to the efficient ones.


\begin{property} [Max spacing over k-clustering]  Given constant $k \in \nat$, a normalizing function $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$  maximizes the spacing over a clustering $\clusterset$ with $k$-clusters.
\end{property}

... Property \ref{prop:ConDis} cannot be satisfied by any methodology in some cases (i.e. set of model evaluations include 0 and 1) together with some other properties. But with the introduction of the clustering related concepts, we define a relaxed version of the property by limiting the restriction within clusters.

\begin{property} [Conservation of distance ordering within clusters] Given a clustering $\clusterset$, the relative order between the distance of pairs of normalized model evaluations is preserved within each cluster $C \in \clusterset$. 
\label{prop:ConDisClus}
\begin{center}
 $\bigtriangleup(d_1, d_2) \geq \bigtriangleup(d_3, d_4) \Longrightarrow 
\bigtriangleup(\sigma_{\dataset}(d_1), \sigma_{\dataset}(d_2)) \geq \bigtriangleup(\sigma_{\dataset}(d_3), \sigma_{\dataset}(d_4))$\\
%,  $d_1, d_2 \in \dataset$, \\
where $d_1, d_2, d_3, d_4 \in C$
\end{center}
\end{property}

After defining the \emph{watered down version} of the property regarding the relative distance ordering, we may define the desired mapping which basically consists of the \emph{non-conflicting} subset of the aforementioned properties.

\begin{definition} [Desired mapping] 
Given a SAF $\saf$, a semantic framework $\sem = \sembodyNew$, the multiset of model evaluations of all arguments $\dataset = \{M(a)|a \in \args$\} and some metric $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$, a normalizing mapping $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ with clustering $\clusterset$ is said to be a desired mapping if it obeys the following properties:
\begin{itemize}
\item Bottom argument (Property 1),
\item Guarantee for the existence of arguments (Property 2),
\item Non-decreasing affect (Property 3),
\item Conservation of the relative ordering (Property 4),
\item Maximum spacing over clustering (Property 6),
\item Conservation of the relative ordering within clusters (Property 7).
\end{itemize}
\end{definition}

%\hspace{20mm}

\subsection{Algorithms for Concrete Normalizing Mapping}

\footnote{In the algorithms the list data structure is utilized instead of sets. I haven't been able to find any reliable source indicating the standard pseudocode representation of lists. I've encountered two examples where they were written with set representation and that's what I have done in this document as well.}

...The premise here is that from Cantor's first uncountability proof, we know that the set of all real number is uncountably infinite. Thus for two arbitrary reel numbers, there is some other reel number whose value falls between those two. In the light of this...



%%%%%%%%%%%%%%%%%%%%%%%%%%%% ALG - 1 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Single-linked normalization}

...Here we had the assumption that K is given as an input to the algorithm. Alternatively, it is possible to modify the algorithm to iterate till a certain threshold of average distance amongst all the clusters is reached. 


\begin{algorithm}
\SetKwData{Left}{left}
\SetKwData{This}{this}
\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}
\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\caption{Single Linked Normalizing Algorithm}
\Input{$\dataset$ (instances set), k (\# of clusters))}
\Output{$\dataset$ (normalized instances set)}
\BlankLine
$cluster\_counter = 0$\;
Clusters $\leftarrow\emptyset$\;
\ForEach{$d_i \in \dataset$}{
$newCluster \leftarrow \{d_i\}$\;
$instance\_counter(newCluster) = 1$\;
$Clusters \leftarrow Clusters \cup newCluster$\;
$cluster\_counter++$\;
}
\ForEach{$C_i, C_j \in Clusters$}{
$Compute$ $\bigtriangleup(C_i, C_j)$
}   
\While{$cluster\_counter \neq k$}{
$(C_m, C_n) = $ two clusters closest together in $Clusters$\;
$Clusters \leftarrow Clusters -\{C_m\}-\{C_n\}\cup\{C_m\cup C_n\}$\;
$cluster\_counter--$\;
\ForEach{$C \in Clusters$}{
$Compute$ $\bigtriangleup(C, (C_m\cup C_n))$\;
}
}
Mid $\leftarrow\emptyset$\;
\ForEach{$C_i, C_{i+1} \in Clusters$}{
$Mid \leftarrow Mid \cup \{(max(C_i) + min(C_{i+1}) / 2)\}$\;
}   
\ForEach{$C_i \in Clusters$\textbackslash $C_k$ }{
\ForEach{$d \in C_i$}{
$d  \leftarrow \dfrac{d - min(C_i)}{max(C_i) - min(C_i)} (Mid[i]  - min(C_i))+ min(C_i)$\;
}
}


\label{algo_singLink}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\emph{special treatment of the first line}\;
\For{$i\leftarrow 2$ \KwTo $l$}{
\emph{special treatment of the first element of line $i$}\;
\For{$j\leftarrow 2$ \KwTo $w$}{\nllabel{forins}
\Left$\leftarrow$ \FindCompress{$Im[i,j-1]$}\;
\Up$\leftarrow$ \FindCompress{$Im[i-1,]$}\;
\This$\leftarrow$ \FindCompress{$Im[i,j]$}\;
\If{\Left compatible with \This}{
\lIf{\Left $<$ \This}{\Union{\Left,\This}}\;
\lElse{\Union{\This,\Left}}
}
\If{\Up compatible with \This}{
\lIf{\Up $<$ \This}{\Union{\Up,\This}}\;
\lElse{\Union{\This,\Up}}
}
}
\lForEach{element $e$ of the line $i$}{
\FindCompress{p}
}
}

\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{remark}

Crudely the normalizing algorithms consist of two parts. The first part uses some clustering method, in which we basically make a partition of the dataset and assign each point to some cluster. But keep in mind that this phase can be considered as a labeling phase, in the sense that the model evaluation values(points) are not modified.

The following step is the update phase, where we normalize the values within clusters. So we may think of the normalizing method as the composition of two methods $\sigma = \gamma o \delta$, the labelling phase $\gamma$ and the update phase $\delta$.

Since the labelling phase does not modify the values, it seems as if we may focus on the mapping that the update phase implements for the proof of the desirable properties, except the max-spacing property which particularly focuses on clustering.

The update phase of the algorithm which contain the normalization routine within clusters is between lines 20 and 28. The objective function can be simply represented as:

\begin{center}
$\delta_{\dataset}(x) = \dfrac{x-min(C_i)}{max(C_i)-min(C_i)} (Mid[i]-min(C_i)) +min(C_i) = \dfrac{x-a}{b-a} (c-a) + a$
\end{center}

where $x$ is the datapoint that goes under normalization, $min(C_i)$\emph{(=a)} and $max(C_i)$\emph{(=b)} are the minumum and maximum elements(respectively) of the cluster $C_i$ that $x$ belongs to, $Mid[i]$\emph{(=c)} is the middle point amongst cluster $C_i$ and $C_{i+1}$.

As mentioned, for the upcoming proofs, except Proposition 5 (regarding Property 6) we will work with this objective(update) function.  Proposition 5 is perhaps the trickiest of all, in which we will be working on the clustering segment of the algorithm.

\end{remark}

\begin{lemma}
Single-linked normalization satisfies Property 1.
\end{lemma}

\begin{proof}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\dataset = \{M(a)|a \in \args$\} be the multiset of model evaluations of all arguments, $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ the function that single-linked normalization algorithm implements and some arbitrary $k \in \mathbb{Z}^{+}$. %$\bigtriangleup: \valueset \times \valueset \rightarrow \real$ a metric,


Let $x \in \dataset$ s.t. $x = 0$. 

Since $x = 0$, it should follow that $x \in C_0$ (As zero is the smallest value in the unit interval).

\begin{align*}
  \sigma_{\dataset}(0) 
  &= \dfrac{-a}{b-a} (c-a) + a
  \\ &=\dfrac{0}{b} c                   \tag{$min(C_0) = a = 0$}
  \\ &= 0                  \>    \qedhere
\end{align*}
 
\end{proof}

\begin{lemma}
Single-linked normalization satisfies Property 2.
\end{lemma}

\begin{proof}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\dataset = \{M(a)|a \in \args$\} be the multiset of model evaluations of all arguments, $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ the function that single-linked normalization algorithm implements, some arbitrary $k \in \mathbb{Z}^{+}$ and assume $\nexists a \in \args$ s.t. $M(a) = 0$. %$\bigtriangleup: \valueset \times \valueset \rightarrow \real$ a metric,

Property states that $x \neq 0 \Rightarrow f(x) \neq 0$. 

We will use proof by contradiction. Let's assume  $x \neq 0$ and $\Rightarrow \sigma_{\dataset}(x) = 0$. Then

\begin{align*}
  \sigma_{\dataset}(x) 
  &= \dfrac{x-a}{b-a} (c-a) + a = 0
  \\ &\Rightarrow  \dfrac{x-a}{b-a} (c-a) = -a    
  \\ &\Rightarrow  x - a= -a \dfrac{b-a}{c-a}  
  \\ &\Rightarrow  x = -a \dfrac{b-a}{c-a} + a
   \\ &\Rightarrow  x = -a   (1 -\dfrac{b-a}{c-a})                                              
\end{align*}

We know that $1 \geq c \geq b \geq a \geq 0$ (i.e. $\frac{min(C_{i+1})+max(C_i)}{2} \geq max(C_i) \geq min(C_i)$). Thus $1 \geq \dfrac{b-a}{c-a} \geq 0$  (for $c \neq a$), and by that we can conclude that in the equality above the value in the parentheses is zero or positive. And finally from that we may conclude $x$ is either zero or negative.

However $x$ cannot be negative since $x \in [0, 1]$ as all datapoints. Furthermore x cannot be zero since our initial assumption was such that  $x \neq 0$.

All in all the initial assumption lead to a \textbf{contradiction}.

\end{proof}

\begin{lemma}
Single-linked normalization satisfies Property 3.
\end{lemma}

\begin{proof}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\dataset = \{M(a)|a \in \args$\} be the multiset of model evaluations of all arguments, $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ the function that single-linked normalization algorithm implements and some arbitrary $k \in \mathbb{Z}^{+}$.

To prove that the objective function is non-decreasing, we should simply show that the derivative is non-negative.

\begin{align*}
  \frac{d}{d(x)} \sigma_{\dataset}(x)
  &= \frac{d}{d(x)} \dfrac{x-a}{b-a} (c-a) + a
 \\ &=  \frac{b-a}{c-a} \geq 0               \>                 \tag{$1 \geq c \geq b \geq a \geq 0$}
\end{align*}

\end{proof}

\begin{lemma}
Single-linked normalization satisfies Property 4.
\end{lemma}

\begin{proof}
We may basically use the proof of Property 3. There we had proven that the objective function $f$ is non-decreasing (i.e. $\dfrac{d}{d(x)} \sigma_{\dataset}(x) \geq 0$).

Thus under the same assumptions, it follows that  $x_1 \geq x_2 \Longrightarrow \sigma_{\dataset}(x_1) \geq \sigma_{\dataset}(x_2)$.

\end{proof}


\begin{lemma}
Single-linked normalization satisfies Property 6.
\end{lemma}

\begin{proof}
In a nutshell in this proof we want to demonstrate that the clustering that Single-linked normalization computes has spacing at least as large as any arbitrary clustering possess. 

Let $\clusterset = C_1,...,C_k$ be the clustering that Single-linked normalization outputs with spacing (i.e. distance between the closest pair of separated datapoint) $S$.

And let $\clusterset' = C_1',...,C_k'$ some arbitrary clustering and has spacing $S'$.

So once again our goal is to show that $S \geq S'$. And we may accomplish this by displaying a pair of points separated by this clustering such that the distance between those separated points is S or smaller.

\textbf{Case 1:} The trivial case is when $C_i'$ 's are the same as the $C_i$ 's, possibly up to a renaming. Thus of course  exactly the same pairs of points are 
separated into each of the clustering, so that the spacing is identical. 

\textbf{Case 2:} The intriguing case is when the two clusterings fundamentally differ. Here we're going to argue  that, for any clustering which is not  merely a permutation of $\clusterset$, there has to be a pair of points which is classified differently in the  $C_i'$ 's relative to the $C_i$ 's. 

By differently we mean that it should be possible to find a point pair $p, q$ such that $p, q$ belong to the same cluster $C_i$ in $\clusterset$. But in the alternative clustering $\clusterset'$ they are placed in different clusters, say $C_i', C_j'$.

So here we may divide the proof into an easy case and a trickier case.

We know that Single-linked normalization works in a greedy sense, i.e. the separated pair of points  that are closest to each other are the 
ones that should get merged. So for this reason, because it's always the closest separated pair that get merged, if we examine at the sequence of 
point pairs that get merged together, that determine the spacing in each subsequent iteration, the distances between these sort of worst separated points is only increasing over time. And this sequence culminates with the final spacing S of the algorithm. In other words, the spacing of the output of the greedy algorithm is the distance between the two clusters that would get merged if we ran the algorithm one more iteration. Thus we know that for any directly merged two points $x, y$,  the distance $\bigtriangleup (x, y) \leq S$.

The easy case occurs the aforementioned point pair $p ,q$ is directly merged at some point. So our algorithm picked them at some iteration (since they constituted the shortest distance amongst separated points)., and their respective clusters were merged.

Then as we have just established, the distance amongst $p$ and $q$ should be no more than $S$. However since $p$ and $q$ are contained in different clusters in $\clusterset'$, they are indeed \textit{separated points} by definition, and thus they constitute an upper bound on $S'$.

Hence we have: $S' \leq \bigtriangleup(p, q) \leq S$. Therefor we've concluded the proof for the easier case of Case 2.
 
The tricky case occurs when $p$ and $q$ are indirectly merged, through a series of direct merges... 
\end{proof}

\begin{lemma}
Single-linked normalization satisfies Property 7.
\end{lemma}

\begin{proof}
Let $\saf$ be a SAF, $\sem = \sembodyNew$ a semantic framework, $\dataset = \{M(a)|a \in \args$\} be the multiset of model evaluations of all arguments, $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ the function that single-linked normalization algorithm(SLN) implements, $\bigtriangleup(x,y) \mapsto |x - y|$ be the metric SLN utilizes, $\clusterset$ be the clustering SNL produces and some arbitrary $k \in \mathbb{Z}^{+}$.

Assume we take four arbitrary datapoints  $x_1, x_2, x_3, x_4 \in C$ from an arbitrary $C in \clusterset$, for which the following statement holds:


\begin{align*}
 & \bigtriangleup(x_1, x_2) \geq \bigtriangleup(x_3, x_4)
 \\ &\Rightarrow |x_1 - x_2| \geq |x_3 - x_4|
 \\ &\Rightarrow |x_1 - x_2 - (a - a)| \geq |x_3 - x_4 - (a - a)| \>                 \tag{$a - a = 0$}
 \\ &\Rightarrow |(x_1 - a) - (x_2  - a)| \geq |(x_3  - a) - (x_4 - a)| 
\\ &\Rightarrow |(x_1 - a) \frac{(c-a)}{(b-a)} - x_2  - a) \frac{(c-a)}{(b-a)}| \geq |(x_3  - a) \frac{(c-a)}{(b-a)} - (x_4 - a) \frac{(c-a)}{(b-a)}|  \tag{ $\frac{(c-a)}{(b-a)} > 0$}
 \\&\Rightarrow  |(\sigma_{\dataset}(x_1) - \sigma_{\dataset}(x_2))| \geq |(\sigma_{\dataset}(x_3), \sigma_{\dataset}(x_4))|					\tag{$\dfrac{x-a}{b-a} (c-a)=\sigma_{\dataset}(x)$}
 \\&\Rightarrow  \bigtriangleup(\sigma_{\dataset}(x_1), \sigma_{\dataset}(x_2)) \geq \bigtriangleup(\sigma_{\dataset}(x_3), \sigma_{\dataset}(x_4))
\end{align*}
\end{proof}

\begin{theorem}
Single-linked normalization is a desired mapping.
\end{theorem}

\begin{proof}
\begin{itemize}
\item Firstly via Lemma 5 we have shown that Single-linked normalization has maximum-spacing. That was the only property we had to prove regarding to the labeling phase.
\item By Remark 1, we had demonstrated the mapping implemented by the Single-linked normalization. In Lemma 1,2,3,4 and 6 we have proved that the particular mapping satisfies properties 1,2,3,4 and 7 respectively.
\end{itemize}

In conjunction, Single-linked normalization satisfied the list of properties considering the class of desired mappings.
\end{proof}

\begin{proposition}
Single-linked normalization has quadratic time-complexity with respect to the number of datapoints (i.e. $\mathcal{O}(|\dataset|^2)$).
\end{proposition}

\begin{proof}
We start the algorithm by creating a distinct cluster from each algorithm, and instantiating the initial cluster list. The time complexity for this procedure is linear wrt. datapoints, i.e.  $\mathcal{O}(|\dataset|)$.

For each pair of points, the distance (metric) is computed. This is indeed an expensive procedure in the algorithm as it has quadratic ($\mathcal{O}(|\dataset|^2)$).

Then closest clusters are merged till the respective input constant value equals the cardinality of the cluster list. This procedure is linear in terms of cluster number. But furthermore at each iteration, the distances with the rest of clusters is computed for the newly merged cluster (again linear). Nested loops result into quadratic behavior  ($\mathcal{O}(k) * \mathcal{O}(|\dataset|) = \mathcal{O}(k|\dataset|)$).

Finally we have two more separate linear procedures, first for computing the middle point amongst consecutive clusters. The second for normalizing each datapoints within its respective clusters.

All in all we have:  $\mathcal{O}(|\dataset|) + \mathcal{O}(|\dataset|^2)  + \mathcal{O}(k|\dataset|)  + \mathcal{O}(|\dataset|) + \mathcal{O}(|\dataset|) =  \mathcal{O}(k|\dataset| + |\dataset|^2) = \mathcal{O}(|\dataset|^2) $  (As $|\dataset| \geq k$).

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%% ALG - 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Additive normalization}


... here our guiding principle is assigning datapoints one at a time to existing clusters, when possible...

\begin{algorithm}
\SetKwData{Left}{left}
\SetKwData{This}{this}
\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}
\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\caption{Additive Normalizing Algorithm}
\Input{$\dataset$ (instances set), $k$ (\# of clusters), $\gamma$ (threshold constant))}
\Output{$\dataset$ (normalized instances set)}
\BlankLine
%$cluster\_counter = 0$\;
Clusters $\leftarrow\emptyset$\;
\ForEach{$d_i \in \dataset$}{
$assigned(d_i) = false$\;
\ForEach{$C \in Clusters$}{
\If{$|d_i - centroid(C)| < \gamma$}{
	$updateCentroid(C)$\;
	$instance\_counter(C)++$;
	$assigned(d_i) = true$\;
	$break$\;
		}
	}


\If{$!assigned(d_i)$}{
$instance\_counter(newCluster) = 1$\;
$centroid(newCluster = d_i)$\;
$Clusters \leftarrow Clusters \cup newCluster$\;

}
}

Mid $\leftarrow\emptyset$\;
\ForEach{$C_i, C_{i+1} \in Clusters$}{
$Mid \leftarrow Mid \cup \{(max(C_i) + min(C_{i+1}) / 2)\}$\;
}   
\ForEach{$C_i \in Clusters$\textbackslash $C_k$}{
\ForEach{$d \in C_i$}{
$d  \leftarrow \dfrac{d - min(C_i)}{max(C_i) - min(C_i)} (Mid[i]  - min(C_i))+ min(C_i)$\;
}
}


\label{algo_incremental}
\end{algorithm}


\begin{proposition}
Additive normalization has time-complexity of  $\mathcal{O}(k|\dataset|)$.
\end{proposition}

\begin{proof}
In the main loop of the algorithm we iterate over all of the datapoints ($\mathcal{O}(|\dataset|)$).  For each point, in the worst case we have to check all existing clusters  ($\mathcal{O}(k)$). Thus the time-complexity of the big for-loop is  $\mathcal{O}(k|\dataset|)$.

As in Algorithm 1, we follow the same procedure for normalizing the values of datapoints within clusters. Hence again we have two isolated procedures that take linear time in terms of the cardinality of the dataset($\mathcal{O}(|\dataset|)$).

The total time-complexity:  $\mathcal{O}(k|\dataset|)  + \mathcal{O}(|\dataset|) + \mathcal{O}(|\dataset|) =  \mathcal{O}(k|\dataset|)$.

\end{proof}





%%%%%%%%%%%%%%%%%%%%%%%%%%%% ALG - 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsubsection{K-means normalization}
...flexible via termination condition, efficient and simple but not space-maximizing...

\begin{algorithm}
\SetKwData{Left}{left}
\SetKwData{This}{this}
\SetKwData{Up}{up}
\SetKwFunction{Union}{Union}
\SetKwFunction{FindCompress}{FindCompress}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\caption{K-means Normalizing Algorithm}
\Input{$\dataset$ (instances set), $k$ (\# of clusters), $t$ (iteration constant))}
\Output{$\dataset$ (normalized instances set)}
\BlankLine
assign $k$ cluster centers $\mu_1, \mu_2,...,\mu_k$ to clusters $C_1, C_2,...C_k$\;
\While{$t  \neq 0$}{
\ForEach{$d_i \in \dataset$}{
assign $d_i$ to the cluster $C$ which has the closest center(mean)\;
}
$Clusters \leftarrow \bigcup^k_{i=1} C_i$\;
\ForEach{$C_i \in Clusters$}{
$\mu_i \leftarrow \dfrac{1}{|C_i|}\sum\limits_{j=1}^{|C_i|} d_j$
}

t--\;
}

Mid $\leftarrow\emptyset$\;
\ForEach{$C_i, C_{i+1} \in Clusters$\textbackslash $C_k$}{
$Mid \leftarrow Mid \cup \{(max(C_i) + min(C_{i+1}) / 2)\}$\;
}   
\ForEach{$C_i \in Clusters$}{
\ForEach{$d \in C_i$}{
$d  \leftarrow \dfrac{d - min(C_i)}{max(C_i) - min(C_i)} (Mid[i]  - min(C_i))+ min(C_i)$\;
}
}


\label{algo_Kmeans}
\end{algorithm}





\begin{proposition}
K-means normalization normalization has time-complexity of  $\mathcal{O}(t(k+|\dataset|))$.
\end{proposition}

\begin{proof}
The main while loop iterates till the termination condition is satisfied ($\mathcal{O}(t))$.

Inside the while loop we have two isolated procedures. First one iterates over each datapoint. Combined with the outer loop, we get $\mathcal{O}(t\dataset)$.

The second iterates over all clusters and updates the mean info. Again combined with the outer loop, we have time complexity of  $\mathcal{O}(tk)$.

As in Algorithm 1, we follow the same procedure for normalizing the values of datapoints within clusters. Hence again we have two isolated procedures that take linear time in terms of the cardinality of the dataset($\mathcal{O}(|\dataset|)$).

The total time-complexity:  $\mathcal{O}(t|\dataset|)  + \mathcal{O}(tk)  + \mathcal{O}(|\dataset|) + \mathcal{O}(|\dataset|) =  \mathcal{O}(tk+t|\dataset|)$.

\end{proof}



\subsection{Discussion}

COMPARISON OF ALGORITHMS COMP/PROP AND GENERAL REMARKS

...the merit of the Single-linked normalization algorithm is easily displayed via property 6. for the other two, the properties that they adhere to constitute a rather flexible class of mappings. so perhaps the merit of these algorithms is not so easily seen. however they create a better valuation for the data. "better how", this should be captured by some property or parameter that can be measured. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment} %sut'un begenmedigi bolum

\subsection{Classes of Normalizing Mappings}
%SLOTA'dan bak

Here we define some concrete classes of normalizing mappings which all adhere to a subset of desired properties defined in the previous section. 

The purpose of this section is just to display that different envisioned uses may require functions that satisfy different collections of properties. 

\begin{definition} [Perfect normalizing mapping] Given a SAF $\saf$ and a semantic framework $\sem = \sembodyNew$,  an injective mapping $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ is a perfect normalizing mapping if $\sigma$ adheres to Properties 1, 2, 3, 4, 5 and 6.
\end{definition}

Property 3 emphasizes the importance of the popular opinion. It carries great significance when defining the formal backbones of the framework. However we may choose to allow the normalized values to go beyond the social support values on the GUI part, with the goal of a more clear display of the argumentation graph. Thus we may choose to not require the particular function to hold for Property 3.

\begin{definition} [GUI-friendly normalizing mapping] Given a SAF $\saf$ and a semantic framework $\sem = \sembodyNew$,  an injective mapping $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ is a perfect normalizing mapping if $\sigma$ adheres to Properties 1, 2, 4, 5 and 6.
\end{definition}

Right now we envision that the process of clustering will play a key role in our generic normalization function. On the other hand surely it will increase the time-complexity of the method. Thus perhaps in situations where we're more concerned with the time the algorithm takes, then the precision of the output, we may define the normalization function without taking the clustering property into account.

\begin{definition}[Fast normalizing mapping] Given a SAF $\saf$ and a semantic framework $\sem = \sembodyNew$,  an injective mapping $\sigma_{\dataset}: \dataset  \rightarrow  [0,1]$ is a perfect normalizing mapping if $\sigma$ adheres to Properties 1, 2, 3, 4 and 5.
\end{definition}

\end{comment}

%BIZIM PAPER CITATION
%\cite{eml2013esaf}

\end{document}